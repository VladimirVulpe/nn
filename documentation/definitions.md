**diskret** = [mathematischen Operationen auf endlichen oder höchstens abzählbar unendlichen Mengen](https://de.wikipedia.org/wiki/Diskrete_Mathematik) <br>
**features** = attributes in the data set that can be used for prediction. These attributes can be **numerical** or **categorical** <br>
**training data set** = machine learns f (Y = f(X) + ε) from labeled training data <br>
**test data set** = machine predicts Y (Y = f(X) + ε) from unlabeled testing data <br> 
**tensor** = [A 1D tensor is a vector (1 row, many columns), a 2D tensor is a matrix (many rows, many columns), and then you can have
tensors with 3, 4, 5 or more dimensions (e.g. a 3D tensor with rows, columns, and depth)](http://www.deeplearningbook.org/contents/linear_algebra.html) <br>
**parametric method**:  means it makes an assumption about the form of the function relating X and Y <br>
**cost function** or **loss function** = [is a function that maps an event or values of one or more variables onto a real number intuitively representing some "cost" associated with the event. In statistics, typically a loss function is used for **parameter estimation**, and the event in question is some function of the difference between estimated and true values for an instance of data.](https://en.wikipedia.org/wiki/Loss_function)  <br>
**overfitting** = learning a function that perfectly explains the training data that the model learned from, but **doesn’t generalize well to unseen test data**. Overfitting happens when a model overlearns from the training data to the point that it starts picking up **idiosyncrasies that aren’t representative of patterns in the real world**. <br>
**bias** = is the amount of error introduced by approximating real-world phenomena with a simplified model. <br> 
**variance** = is how much your model's test error changes based on variation in the training data. It reflects the model's sensitivity to the idiosyncrasies of the data set it was trained on.<br> 

**regularization** =  <br>
**Convulational Neural Network (CNN)** = <br>
 <br>
 <br>
 <br>
 <br>
